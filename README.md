# ğŸš€ Benchmarking Optimization Algorithms (SGD, Momentum, NAG)

## ğŸŒŸ Project Highlights

âœ” Implemented core optimization algorithms from scratch (no ML libraries)  
âœ” Visualized optimizer trajectories on complex non-convex loss surfaces  
âœ” Benchmarked convergence speed and stability across multiple functions  
âœ” Compared SGD, Momentum, and Nesterov Accelerated Gradient (NAG)  
âœ” Demonstrated practical understanding of gradient dynamics in ML  

---

## ğŸ“Œ Overview

This project benchmarks and compares the performance of different gradient-based optimization algorithms used in machine learning. The goal is to analyze how quickly and effectively each optimizer converges while minimizing loss.

The optimizers implemented and evaluated in this project include:

- Stochastic Gradient Descent (SGD)
- Momentum
- Nesterov Accelerated Gradient (NAG)

---

## ğŸ¯ Objective

The main objectives of this project are:

- To implement core optimization algorithms from scratch  
- To compare their convergence behaviour  
- To analyze optimization speed and stability  
- To visualize loss reduction across iterations  

---

## âš™ï¸ Optimizers Implemented

### 1. Stochastic Gradient Descent (SGD)
The basic optimization algorithm that updates parameters using the gradient of the loss function.

### 2. Momentum
Momentum accelerates SGD by accumulating past gradients to smooth updates and reduce oscillations.

### 3. Nesterov Accelerated Gradient (NAG)
NAG improves momentum by computing the gradient at the â€œlook-aheadâ€ position, resulting in faster and more informed updates.

---

## ğŸ§® Mathematical Formulation

Let Î¸ be model parameters and L(Î¸) be the loss function.

**SGD update:**

Î¸ = Î¸ âˆ’ Î· âˆ‡L(Î¸)

**Momentum update:**

vâ‚œ = Î³ vâ‚œâ‚‹â‚ + Î· âˆ‡L(Î¸)  
Î¸ = Î¸ âˆ’ vâ‚œ

**Nesterov Accelerated Gradient (NAG):**

vâ‚œ = Î³ vâ‚œâ‚‹â‚ + Î· âˆ‡L(Î¸ âˆ’ Î³ vâ‚œâ‚‹â‚)  
Î¸ = Î¸ âˆ’ vâ‚œ

---

## ğŸ§ª Experimental Setup

- Language: Python  
- Libraries: NumPy, Matplotlib  
- Training approach: Iterative gradient updates  
- Evaluation metric: Loss vs iterations  

---

## ğŸ§ª Benchmark Functions

The optimizers were evaluated on standard non-convex test functions:

- **Himmelblau Function** â€“ multiple local minima  
- **Levy Function** â€“ highly complex landscape  
- **Anisotropic Function** â€“ ill-conditioned curvature  
- **MatÃ©rn Function** â€“ smooth but non-trivial surface  

---

## ğŸ“Š Results & Visualization

### Loss vs Iterations

#### Anisotropic Function
![Loss Comparison](results/loss-iteration/anisotrophic-loss.png)

#### Himmelblau Function
![Loss Comparison](results/loss-iteration/himmelblau-loss.png)

#### Levy Function
![Loss Comparison](results/loss-iteration/levy-loss.png)

---

### Optimization Paths

#### Anisotropic Surface
![Optimizer Paths](results/paths/anisotrophic.png)

#### Himmelblau Surface
![Optimizer Paths](results/paths/himmelblau.png)

#### MatÃ©rn Surface
![Optimizer Paths](results/paths/matern.png)

---

## ğŸ“Š Quantitative Results (Sample Summary)

| Optimizer | Function    | Iterations to Converge | Final Loss | Stability |
|------------|-------------|------------------------|------------|-----------|
| SGD        | Himmelblau  | ~1800                  | Low        | Medium    |
| Momentum   | Himmelblau  | ~950                   | Lower      | High      |
| NAG        | Himmelblau  | ~375                   | Lowest     | High      |

---

## ğŸ” Key Observations

- NAG converges faster compared to SGD and Momentum  
- Momentum provides smoother convergence than SGD  
- SGD is simple but slower in reaching optimal loss  
- NAGâ€™s look-ahead gradient leads to better trajectory planning  
- Performance gain is more visible on ill-conditioned surfaces  

---

## ğŸ¤” When to Use Which Optimizer?

- **SGD** â†’ Simple problems, low memory usage  
- **Momentum** â†’ Noisy gradients, oscillating loss surfaces  
- **NAG** â†’ Faster convergence and better directional awareness  

---

## ğŸ“ Project Structure

```
Benchmark-estimation-optimizers/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ benchMark-OPTMISER.ipynb    
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ loss-iteration/        
â”‚   â””â”€â”€ paths/                  
â”‚
â””â”€â”€ README.md                  
```



## ğŸš€ Future Improvements

- Add Adam and RMSProp optimizers
- Compare performance across different learning rates
- Benchmark on real datasets (e.g., MNIST)
- Measure computation time vs accuracy trade-off

---

## ğŸ’¼ Relevance

This project demonstrates practical understanding of:

- Optimization in Machine Learning
- Gradient-based learning methods
- Performance benchmarking
- Numerical experimentation and analysis


---

## ğŸ‘¨â€ğŸ’» Author

**Vishnu Vardhan Kasireddy**  
GitHub: https://github.com/VishnuVardhanKasireddy

---

## â­ If you found this useful
Give the repo a star â­ and feel free to fork or contribute!
