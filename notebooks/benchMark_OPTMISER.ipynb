{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaYCCYFTySJgdCGDoOeFcn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishnuVardhanKasireddy/Benchmark-estimation-optimizers/blob/main/benchMark_OPTMISER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "metadata": {
        "id": "gvspzdnl0efv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Rosenbrock Function\n",
        "# ------------------------------------------------------------\n",
        "def rosenbrock(x, y):\n",
        "    return (1-x)**2 + 100*(y - x**2)**2\n",
        "\n",
        "def rosenbrock_grad(x, y):\n",
        "    dx = -2*(1-x) - 400*x*(y - x**2)\n",
        "    dy = 200*(y - x**2)\n",
        "    return np.array([dx, dy])"
      ],
      "metadata": {
        "id": "eUei_2X40c4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# Saddle-Point Funcion\n",
        "# --------------------------------------------\n",
        "def saddle(x,y):\n",
        "  return x**2-y**2\n",
        "def saddle_grad(x,y):\n",
        "  dx=2*x\n",
        "  dy=-2*y\n",
        "  return np.array([dx,dy])\n"
      ],
      "metadata": {
        "id": "xtq0hC6h1UFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------\n",
        "# Anisotrophic-Quadratic Function --------------\n",
        "# ----------------------------------------------\n",
        "def aniso(x,y):\n",
        "  return 0.5*(1729*x**2+3145*y**2)\n",
        "def aniso_grad(x,y):\n",
        "  dx=100*x\n",
        "  dy=1000*y\n",
        "  return np.array([dx,dy])\n"
      ],
      "metadata": {
        "id": "GEb_QBn_3eds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Simple-Quadratic Function\n",
        "# -----------------------------\n",
        "def simple(x,y):\n",
        "  return 0.5*(x*y)\n",
        "def simple_grad(x,y):\n",
        "  dx=0.5*y\n",
        "  dy=0.5*x\n",
        "  return np.array([dx,dy])"
      ],
      "metadata": {
        "id": "uzmTDZel5zga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Himmelblau's-Function -----------------\n",
        "# ---------------------------------------\n",
        "def himmelblau(x,y):\n",
        "  return (x**2+y+11)**2+(x+y**2+7)**2\n",
        "def himmelblau_grad(x,y):\n",
        "  dx=2*(x**2+y+11)*2*x+2*(x+y**2+7)\n",
        "  dy=2*(x**2+y+11)+2*(x+y**2+7)*2*y\n",
        "  return np.array([dx,dy])"
      ],
      "metadata": {
        "id": "899W4Wnr7Ukv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Bukin Function\n",
        "# -------------------------------------\n",
        "def bukin6(x, y):\n",
        "    return 100*np.sqrt(np.abs(y - 0.01*(x**2))) + 0.01*np.abs(x + 10)\n",
        "\n",
        "def bukin6_grad( x, y,f=bukin6, eps=1e-6):\n",
        "    dx = (f(x+eps, y) - f(x-eps, y)) / (2*eps)\n",
        "    dy = (f(x, y+eps) - f(x, y-eps)) / (2*eps)\n",
        "    return np.array([dx, dy])\n"
      ],
      "metadata": {
        "id": "8Fj68f4VEDQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# Rastring Function\n",
        "# ----------------------------------------\n",
        "def rastrigin(x, y):\n",
        "    return 20 + (x**2 - 10*np.cos(2*np.pi*x)) + (y**2 - 10*np.cos(2*np.pi*y))\n",
        "def rastrigin_grad(x, y):\n",
        "    dx = 2*x + 20*np.pi*np.sin(2*np.pi*x)\n",
        "    dy = 2*y + 20*np.pi*np.sin(2*np.pi*y)\n",
        "    return np.array([dx, dy])"
      ],
      "metadata": {
        "id": "_nKwJZCnDozE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------\n",
        "# Six-Hump Function ---------------------\n",
        "# ---------------------------------------\n",
        "def six_hump(x, y):\n",
        "    return (4 - 2.1*x**2 + (x**4)/3)*x**2 + x*y + (-4 + 4*y**2)*y**2\n",
        "\n",
        "def six_hump_grad(x, y):\n",
        "    dx = (8*x - 4.2*x**3 + (4/3)*x**5) + y\n",
        "    dy = x + (-8*y + 16*(y**3))\n",
        "    return np.array([dx, dy])\n"
      ],
      "metadata": {
        "id": "bEWa0HVBFR08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# Levy Function ----------------------\n",
        "# ------------------------------------------\n",
        "def levy(x, y):\n",
        "    w1 = 1 + (x - 1)/4\n",
        "    w2 = 1 + (y - 1)/4\n",
        "\n",
        "    term1 = np.sin(np.pi * w1)**2\n",
        "    term2 = (w1 - 1)**2 * (1 + 10 * np.sin(np.pi*w1 + np.pi)**2)\n",
        "    term3 = (w2 - 1)**2 * (1 + np.sin(2*np.pi*w2)**2)\n",
        "\n",
        "    return term1 + term2 + term3\n",
        "\n",
        "def levy_grad(x, y, eps=1e-4, clip=5.0):\n",
        "    dx = (levy(x + eps, y) - levy(x - eps, y)) / (2 * eps)\n",
        "    dy = (levy(x, y + eps) - levy(x, y - eps)) / (2 * eps)\n",
        "\n",
        "    g = np.array([dx, dy])\n",
        "\n",
        "    # Gradient clipping for stability\n",
        "    norm = np.linalg.norm(g)\n",
        "    if norm > clip:\n",
        "        g = g / norm * clip\n",
        "\n",
        "    return g\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQyim05iGFgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# Ackley Function------------------------\n",
        "# -----------------------------------------\n",
        "def ackley(x, y):\n",
        "    term1 = -20 * np.exp(-0.2 * np.sqrt(0.5 * (x**2 + y**2)))\n",
        "    term2 = -np.exp(0.5 * (np.cos(2*np.pi*x) + np.cos(2*np.pi*y)))\n",
        "    return term1 + term2 + np.e + 20\n",
        "\n",
        "def ackley_grad(x, y):\n",
        "    # Common terms\n",
        "    r = np.sqrt(0.5 * (x**2 + y**2))\n",
        "    exp1 = np.exp(-0.2 * r)\n",
        "    exp2 = np.exp(0.5 * (np.cos(2*np.pi*x) + np.cos(2*np.pi*y)))\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if r == 0:\n",
        "        dx1 = dy1 = 0\n",
        "    else:\n",
        "        dx1 = (x / r) * exp1 * 2\n",
        "        dy1 = (y / r) * exp1 * 2\n",
        "\n",
        "    dx2 = exp2 * np.pi * np.sin(2*np.pi*x)\n",
        "    dy2 = exp2 * np.pi * np.sin(2*np.pi*y)\n",
        "\n",
        "    dx = dx1 + dx2\n",
        "    dy = dy1 + dy2\n",
        "    return np.array([dx, dy])\n"
      ],
      "metadata": {
        "id": "iD1eDIcMF0iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Egg-Holder function\n",
        "# ------------------------------------------\n",
        "def eggholder(x, y):\n",
        "    return -(y + 47) * np.sin(np.sqrt(np.abs(x/2 + y + 47))) \\\n",
        "           - x * np.sin(np.sqrt(np.abs(x - (y + 47))))\n",
        "\n",
        "def eggholder_grad(x, y, eps=1e-6, clip=5.0):\n",
        "    dx = (eggholder(x + eps, y) - eggholder(x - eps, y)) / (2 * eps)\n",
        "    dy = (eggholder(x, y + eps) - eggholder(x, y - eps)) / (2 * eps)\n",
        "\n",
        "    g = np.array([dx, dy])\n",
        "\n",
        "    # Gradient clipping\n",
        "    norm = np.linalg.norm(g)\n",
        "    if norm > clip:\n",
        "        g = g / norm * clip\n",
        "\n",
        "    return g\n"
      ],
      "metadata": {
        "id": "sh8JctuZyo4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------\n",
        "# Cross in tray Function\n",
        "# ----------------------------------------------\n",
        "def cross_in_tray(x, y):\n",
        "    exp_term = np.abs(100 - np.sqrt(x**2 + y**2)/np.pi)\n",
        "    return -0.0001 * (np.abs(np.sin(x)*np.sin(y)*np.exp(exp_term)) + 1)**0.1\n",
        "def cross_in_tray_grad(x, y, eps=1e-4, clip=5.0):\n",
        "    dx = (cross_in_tray(x + eps, y) - cross_in_tray(x - eps, y)) / (2 * eps)\n",
        "    dy = (cross_in_tray(x, y + eps) - cross_in_tray(x, y - eps)) / (2 * eps)\n",
        "\n",
        "    g = np.array([dx, dy])\n",
        "\n",
        "    # Gradient clipping for stability\n",
        "    norm = np.linalg.norm(g)\n",
        "    if norm > clip:\n",
        "        g = g / norm * clip\n",
        "\n",
        "    return g\n",
        "\n"
      ],
      "metadata": {
        "id": "4zw-OACr1tFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------\n",
        "# styblinski tang function\n",
        "# ----------------------------------------\n",
        "def styblinski_tang(x, y):\n",
        "    return 0.5 * (\n",
        "        (x**4 - 16*x**2 + 5*x) +\n",
        "        (y**4 - 16*y**2 + 5*y)\n",
        "    )\n",
        "\n",
        "def styblinski_tang_grad(x, y):\n",
        "    dx = 0.5 * (4*x**3 - 32*x + 5)\n",
        "    dy = 0.5 * (4*y**3 - 32*y + 5)\n",
        "    return np.array([dx, dy])\n"
      ],
      "metadata": {
        "id": "8UnrfIko2_oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------\n",
        "# michalewicz function\n",
        "# ------------------------------------\n",
        "def michalewicz(x, y, m=10):\n",
        "    return -(\n",
        "        np.sin(x) * (np.sin(x**2/np.pi))**(2*m) +\n",
        "        np.sin(y) * (np.sin(2*y**2/np.pi))**(2*m)\n",
        "    )\n",
        "\n",
        "def michalewicz_grad(x, y, eps=1e-4, clip=2.0):\n",
        "    dx = (michalewicz(x+eps, y) - michalewicz(x-eps, y)) / (2*eps)\n",
        "    dy = (michalewicz(x, y+eps) - michalewicz(x, y-eps)) / (2*eps)\n",
        "\n",
        "    g = np.array([dx, dy])\n",
        "    n = np.linalg.norm(g)\n",
        "    if n > clip:\n",
        "        g = g / n * clip\n",
        "    return g\n"
      ],
      "metadata": {
        "id": "oTYmCIni4wao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------\n",
        "# Matern Benchmark Function (ν = 1.5)--\n",
        "# ---------------------------------------\n",
        "\n",
        "def matern(x, y, nu=1.5, x0=0.0, y0=0.0):\n",
        "    r = np.sqrt((x - x0)**2 + (y - y0)**2)\n",
        "    sqrt5_r = np.sqrt(5) * r\n",
        "\n",
        "    term = (1 + sqrt5_r/nu + (5 * r**2)/(3 * nu**2))\n",
        "    Z = -term * np.exp(-sqrt5_r/nu)\n",
        "    Z = np.where(r == 0, 0.0, Z)\n",
        "    return Z\n",
        "\n",
        "\n",
        "# Gradient of the Matern function\n",
        "def matern_grad(x, y, nu=1.5, x0=0.0, y0=0.0):\n",
        "    r = np.sqrt((x - x0)**2 + (y - y0)**2)\n",
        "    sqrt5 = np.sqrt(5)\n",
        "    sqrt5_r = sqrt5 * r\n",
        "\n",
        "    A = (1 + sqrt5_r/nu + (5 * r**2)/(3 * nu**2))\n",
        "    exp_term = np.exp(-sqrt5_r/nu)\n",
        "    dA_dr = (sqrt5/nu) + (10*r)/(3*nu**2)\n",
        "\n",
        "    df_dr = - (dA_dr * exp_term - A * exp_term * (sqrt5/nu))\n",
        "\n",
        "    # Avoid division by zero\n",
        "    r_safe = np.where(r == 0, 1.0, r)\n",
        "\n",
        "    dx = df_dr * (x - x0) / r_safe\n",
        "    dy = df_dr * (y - y0) / r_safe\n",
        "\n",
        "    dx = np.where(r == 0, 0.0, dx)\n",
        "    dy = np.where(r == 0, 0.0, dy)\n",
        "\n",
        "    return np.array([dx, dy])\n"
      ],
      "metadata": {
        "id": "1bkriblwCgVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BENCHMARKS = {\n",
        "    \"rosenbrock\":(rosenbrock,rosenbrock_grad),\n",
        "    \"saddle\": (saddle, saddle_grad),\n",
        "    \"anisotropic\": (aniso, aniso_grad),\n",
        "    \"simple\": (simple, simple_grad),\n",
        "    \"himmelblau\": (himmelblau, himmelblau_grad),\n",
        "    \"bukin6\": (bukin6, bukin6_grad),\n",
        "    \"rastrigin\": (rastrigin, rastrigin_grad),\n",
        "    \"six_hump\": (six_hump, six_hump_grad),\n",
        "    \"levy\": (levy, levy_grad),\n",
        "    \"ackley\": (ackley, ackley_grad),\n",
        "    \"eggholder\": (eggholder, eggholder_grad),\n",
        "    \"cross_in_tray\": (cross_in_tray, cross_in_tray_grad),\n",
        "    \"styblinski_tang\": (styblinski_tang, styblinski_tang_grad),\n",
        "    \"michalewicz\": (michalewicz, michalewicz_grad),\n",
        "    \"matern\": (matern, matern_grad),\n",
        "}\n"
      ],
      "metadata": {
        "id": "rMpWbvk18czX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eyR8ujxy9t2"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(func, grad, x0, lr=1e-4, steps=1000):\n",
        "    x = np.array(x0, dtype=float)\n",
        "\n",
        "    path = [x.copy()]\n",
        "    loss_history = []\n",
        "    grad_norm_history = []\n",
        "    step_norm_history = []\n",
        "\n",
        "    for _ in range(steps):\n",
        "        g = grad(x[0], x[1])\n",
        "        x_new = x - lr * g\n",
        "\n",
        "        path.append(x_new.copy())\n",
        "        loss_history.append(func(x[0], x[1]))\n",
        "        grad_norm_history.append(np.linalg.norm(g))\n",
        "        step_norm_history.append(np.linalg.norm(x_new - x))\n",
        "\n",
        "        x = x_new\n",
        "        if not np.all(np.isfinite(x_new)):\n",
        "          print(\"Diverged. Stopping early.\")\n",
        "          break\n",
        "\n",
        "    return (np.array(path),\n",
        "            np.array(loss_history),\n",
        "            np.array(grad_norm_history),\n",
        "            np.array(step_norm_history))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adam_opt(func, grad, x0, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8, steps=1000, clip=100):\n",
        "    x = np.array(x0, dtype=float)\n",
        "\n",
        "    m = np.zeros_like(x)\n",
        "    v = np.zeros_like(x)\n",
        "\n",
        "    path = [x.copy()]\n",
        "    loss_history = []\n",
        "    grad_norm_history = []\n",
        "    step_norm_history = []\n",
        "\n",
        "    for t in range(1, steps + 1):\n",
        "        g = grad(x[0], x[1])\n",
        "        g = np.clip(g, -clip, clip)\n",
        "\n",
        "        m = beta1 * m + (1 - beta1) * g\n",
        "        v = beta2 * v + (1 - beta2) * (g ** 2)\n",
        "\n",
        "        m_hat = m / (1 - beta1 ** t)\n",
        "        v_hat = v / (1 - beta2 ** t)\n",
        "\n",
        "        x_new = x - lr * m_hat / (np.sqrt(v_hat) + eps)\n",
        "\n",
        "        if not np.all(np.isfinite(x_new)):\n",
        "            print(\"Adam diverged. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        path.append(x_new.copy())\n",
        "        loss_history.append(func(x[0], x[1]))\n",
        "        grad_norm_history.append(np.linalg.norm(g))\n",
        "        step_norm_history.append(np.linalg.norm(x_new - x))\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return (\n",
        "        np.array(path),\n",
        "        np.array(loss_history),\n",
        "        np.array(grad_norm_history),\n",
        "        np.array(step_norm_history),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "LSYEGYN6hUe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_opt(func, grad, x0, lr=5e-5, mu=0.9, steps=3000):\n",
        "    x = np.array(x0, dtype=float)\n",
        "    v = np.zeros_like(x)\n",
        "\n",
        "    path = [x.copy()]\n",
        "    loss_history = []\n",
        "    grad_norm_history = []\n",
        "    step_norm_history = []\n",
        "\n",
        "    for _ in range(steps):\n",
        "        look_ahead = x + mu * v\n",
        "        g = grad(look_ahead[0], look_ahead[1])\n",
        "        v = mu * v - lr * g\n",
        "        x_new = x + v\n",
        "\n",
        "        path.append(x_new.copy())\n",
        "        loss_history.append(func(x[0], x[1]))\n",
        "        grad_norm_history.append(np.linalg.norm(g))\n",
        "        step_norm_history.append(np.linalg.norm(x_new - x))\n",
        "\n",
        "        x = x_new\n",
        "        if not np.all(np.isfinite(x_new)):\n",
        "          print(\"Diverged. Stopping early.\")\n",
        "          break\n",
        "\n",
        "    return (np.array(path),\n",
        "            np.array(loss_history),\n",
        "            np.array(grad_norm_history),\n",
        "            np.array(step_norm_history))\n"
      ],
      "metadata": {
        "id": "wbg36XWWhXc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(func, grad, x0, optimizers_dict, steps=100):\n",
        "    results = {}\n",
        "\n",
        "    for name, optimizer_fn in optimizers_dict.items():\n",
        "        path, losses, grad_norms, step_norms = optimizer_fn(\n",
        "            func, grad, x0, steps=steps\n",
        "        )\n",
        "\n",
        "        results[name] = {\n",
        "            \"path\": path,\n",
        "            \"loss\": losses,\n",
        "            \"grad_norm\": grad_norms,\n",
        "            \"step_norm\": step_norms\n",
        "        }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "NdFBBwcPhjwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZERS = {\n",
        "    \"GD\": gradient_descent,\n",
        "    \"ADAM\": adam_opt,\n",
        "    \"NAG\": nesterov_opt\n",
        "}\n"
      ],
      "metadata": {
        "id": "0Ops_DbuhlVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_comparison_row(results, title_prefix=\"\", log_loss=True):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    # ---- Loss vs Iterations ----\n",
        "    for name, data in results.items():\n",
        "        axes[0].plot(data[\"loss\"], label=name)\n",
        "\n",
        "    if log_loss:\n",
        "        axes[0].set_yscale(\"log\")\n",
        "\n",
        "    axes[0].set_title(f\"{title_prefix} Loss vs Iterations\")\n",
        "    axes[0].set_xlabel(\"Iteration\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].grid(True)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # ---- Gradient Norm vs Iterations ----\n",
        "    for name, data in results.items():\n",
        "        axes[1].plot(data[\"grad_norm\"], label=name)\n",
        "\n",
        "    axes[1].set_title(f\"{title_prefix} Gradient Norm vs Iterations\")\n",
        "    axes[1].set_xlabel(\"Iteration\")\n",
        "    axes[1].set_ylabel(\"||∇f(x)||\")\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # ---- Step Size vs Iterations ----\n",
        "    for name, data in results.items():\n",
        "        axes[2].plot(data[\"step_norm\"], label=name)\n",
        "\n",
        "    axes[2].set_title(f\"{title_prefix} Step Size vs Iterations\")\n",
        "    axes[2].set_xlabel(\"Iteration\")\n",
        "    axes[2].set_ylabel(\"||x(t+1) - x(t)||\")\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "NSz-AHpz65ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_2d_and_3d_equal(func, results, xlim, ylim, title=\"Optimizer Paths\", zscale=1.0):\n",
        "    x = np.linspace(xlim[0], xlim[1], 350)\n",
        "    y = np.linspace(ylim[0], ylim[1], 350)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = np.vectorize(lambda a, b: func(a, b))(X, Y) * zscale\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 7))\n",
        "    gs = gridspec.GridSpec(1, 2, width_ratios=[2, 1])  # equal real estate\n",
        "\n",
        "    # ------------------ 2D Contour ------------------\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    ax1.contour(X, Y, Z, levels=50, cmap=\"viridis\")\n",
        "\n",
        "    for name, data in results.items():\n",
        "        path = data[\"path\"]\n",
        "        ax1.plot(path[:,0], path[:,1], linewidth=2, label=name)\n",
        "        ax1.scatter(path[0,0], path[0,1], s=50, c='k')\n",
        "        ax1.scatter(path[-1,0], path[-1,1], s=50)\n",
        "\n",
        "    ax1.set_xlim(xlim)\n",
        "    ax1.set_ylim(ylim)\n",
        "    ax1.set_aspect('equal', adjustable='box')   # CRITICAL\n",
        "    ax1.set_title(\"2D Contours\")\n",
        "    ax1.set_xlabel(\"x\")\n",
        "    ax1.set_ylabel(\"y\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # ------------------ 3D Surface ------------------\n",
        "    ax2 = fig.add_subplot(gs[1], projection='3d')\n",
        "    ax2.plot_surface(\n",
        "        X, Y, Z,\n",
        "        alpha=0.75,\n",
        "        linewidth=0.1,\n",
        "        antialiased=True,\n",
        "        cmap=\"viridis\"\n",
        "    )\n",
        "\n",
        "    for name, data in results.items():\n",
        "        path = data[\"path\"]\n",
        "        Zp = np.array([func(p[0], p[1]) for p in path]) * zscale\n",
        "        ax2.plot(path[:,0], path[:,1], Zp, linewidth=3, label=name)\n",
        "        ax2.scatter(path[0,0], path[0,1], Zp[0], s=60, c='k')\n",
        "\n",
        "    xlim=(xlim)\n",
        "    ylim=(ylim)\n",
        "    ax2.set_xlim(xlim)\n",
        "    ax2.set_ylim(ylim)\n",
        "\n",
        "    ax2.set_box_aspect((1, 1, 1))\n",
        "\n",
        "    ax2.set_title(\"3D Surface\")\n",
        "    ax2.set_xlabel(\"x\")\n",
        "    ax2.set_ylabel(\"y\")\n",
        "    ax2.set_zlabel(\"z = f(x, y)\")\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DiO9kDzeB8a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def symmetric_bounds(results, pad=2):\n",
        "    xs, ys = [], []\n",
        "    for data in results.values():\n",
        "        path = data[\"path\"]\n",
        "        xs.extend(path[:,0])\n",
        "        ys.extend(path[:,1])\n",
        "\n",
        "    xmid = (min(xs) + max(xs)) / 2\n",
        "    ymid = (min(ys) + max(ys)) / 2\n",
        "\n",
        "    xr = max(xs) - min(xs)\n",
        "    yr = max(ys) - min(ys)\n",
        "\n",
        "    r = max(xr, yr) / 2 + pad\n",
        "\n",
        "    return (xmid - r, xmid + r), (ymid - r, ymid + r)\n"
      ],
      "metadata": {
        "id": "eUDpkcqY8prq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = {}\n",
        "\n",
        "for name, (func, grad) in BENCHMARKS.items():\n",
        "    print(f\"Running benchmark: {name}\")\n",
        "\n",
        "    x0 = np.random.uniform(-5, 5, size=2) # default start\n",
        "    results = run_benchmark(func, grad, x0, OPTIMIZERS, steps=2000)\n",
        "\n",
        "    all_results[name] = results\n",
        "\n",
        "    # ---- Standardized plots ----\n",
        "    plot_comparison_row(results, title_prefix=name)\n",
        "    (xlim, ylim) = symmetric_bounds(results)\n",
        "\n",
        "\n",
        "    plot_2d_and_3d_equal(\n",
        "    func,\n",
        "    results,\n",
        "    xlim=xlim,\n",
        "    ylim=ylim,\n",
        "    zscale=10,\n",
        "    title=f\"{name}: Optimizer Paths\"\n",
        "    )\n",
        "    for name, data in results.items():\n",
        "      loss = data[\"loss\"]\n",
        "      print(name, \"min:\", f\"{np.nanmin(loss):.2f}\",\n",
        "        \"max:\", f\"{np.nanmax(loss):.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8JgjgJI_iQBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O--RW9ZX5GC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# Running all optimizers\n",
        "# ------------------------------------------------------------\n",
        "function_name=\"rosenbrock\"\n",
        "f,grad=BENCHMARKS[function_name]\n",
        "\n",
        "\n",
        "xmin,xmax=-2,3\n",
        "ymin,ymax=-2,3\n",
        "x0=np.random.uniform(xmin,xmax)\n",
        "y0=np.random.uniform(ymin,ymax)\n",
        "start=np.array([x0,y0])\n",
        "\n",
        "path_gd = gradient_descent(grad,start)\n",
        "path_mom = momentum_opt(grad,start)\n",
        "path_nag = nesterov_opt(grad,start)"
      ],
      "metadata": {
        "id": "UkDGjMg9N-gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 3D Plot of the Function + Paths\n",
        "# ------------------------------------------------------------\n",
        "X = np.linspace(-2, 2, 400)\n",
        "Y = np.linspace(-1, 3, 400)\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = f(X, Y) * 10\n",
        "\n",
        "fig = plt.figure(figsize=(20,10 ))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.plot_surface(X, Y, Z, alpha=0.75, linewidth=0.1, antialiased=True)\n",
        "ax.plot(path_gd[:,0], path_gd[:,1], f(path_gd[:,0], path_gd[:,1]), 'r', label=\"GD\")\n",
        "ax.plot(path_mom[:,0], path_mom[:,1], f(path_mom[:,0], path_mom[:,1]), 'b', label=\"Momentum\")\n",
        "ax.plot(path_nag[:,0], path_nag[:,1], f(path_nag[:,0], path_nag[:,1]), 'g', label=\"NAG\")\n",
        "\n",
        "ax.scatter(start[0], start[1], f(start[0], start[1]),c='k', s=80, label='Start')\n",
        "\n",
        "ax.set_title(\"3D Optimization Paths on Function\")\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "ax.set_zlabel(\"z=function(x, y)\")\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "quYGshl9z94A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 2D Contour Comparison\n",
        "# ------------------------------------------------------------\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.contour(X, Y, Z, levels=100)\n",
        "\n",
        "plt.plot(path_gd[:,0], path_gd[:,1], 'r-', label='Gradient Descent')\n",
        "\n",
        "plt.plot(path_mom[:,0], path_mom[:,1], 'b-', label='Momentum')\n",
        "plt.plot(path_nag[:,0], path_nag[:,1], 'g-', label='NAG')\n",
        "\n",
        "plt.scatter(path_gd[0,0], path_gd[0,1], c='black', label='Start')\n",
        "plt.title(\"2D Contour Plot: GD vs Momentum vs NAG\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z6h-TzC5z55m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FliorgZ7BFZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
